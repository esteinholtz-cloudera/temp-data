{
  "Speaker": "Ilya Sutskever",
  "Start": "00:03:10",
  "End": "00:04:50",
  "Text": ", I can answer that question at some point in about 2010 or 2011. connected two facts in my mind. Basically, realization was this some point, we realize that we can train very large. I shouldn't say very, you know, they were tiny by today's standards, but large and deep neural networks end to end with back propagation. At some point. people obtain this result. I obtained this result. The first, the first moment in which I realized that neural networks are powerful was when James Martins invented the Hessian free optimizer in 2010. And he trained a 10 layer neural network end to end without pre training scratch. when that happened, I thought this is it because if you can train a big neural network, a big neural network can represent very complicated function. Because if you have a neural network with 10 layers, as though you allow the human brain to run for number of milliseconds, neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times. So it's also kind of like 10 layers and in 100 milliseconds, you can perfectly recognize any object. So I thought so I already had the idea then that we need to train a very big neural network lots of supervised data. And then it must succeed because we can find the best neural network. And then there's also theory that if you have more data than parameters, you won over ft, we know that actually this theory is very incomplete and you want toe even when you have less data than parameters. But definitely if you have more data than parameters you want over fed. So the"
}