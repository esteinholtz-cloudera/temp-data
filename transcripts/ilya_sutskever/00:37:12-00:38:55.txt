{
  "Speaker": "Ilya Sutskever",
  "Start": "00:37:12",
  "End": "00:38:55",
  "Text": "? OK, great. So you can, you can look, you can things like you can take a neural network you can start increasing its size slowly while keeping your data set fixed. if you increase the size of the neural network slowly and if you don't do early stopping, that's a pretty important detail. when the neural this book is really small, you make it larger, you get a very rapid increase in performance, you continue to make it larger and at some point performance will get worse it gets and and it gets the worst exactly at the point at which it achieves zero training precisely zero training loss. then as you make it large, it starts to get better again. it's kind of counterintuitive because you'd expect deep learning phenomena to be monotonic. hard to be sure what it means. But it also occurs in the case of linear classifiers. And the intuition basically boils down to the following. you, when you have a lot, when you have a large data set and a small model, then small tiny random. So basically what is overfitting, overfitting is when your model is somehow very sensitive to the small random important stuff in your data set in the training in the training data set precisely. So if you have a small model and you have a big data set there may be some random thing, you know, some training cases are randomly in the data set and others may not be there. But the small model, but the small model is kind of insensitive to this randomness because the same, there is pretty much no uncertainty about the model when it is that it's large."
}