{
  "Speaker": "Andrej Karpathy",
  "Start": "00:47:25",
  "End": "00:49:00",
  "Text": "is uh yeah. So language model just uh basically, the rough idea is um just predicting the next word in a sequence, roughly speaking. Uh So there's a paper from, for example, uh Benjo uh and the team from 2003 where for the first time they were using a neural network to take say like three or five words and predict the um next word. they're doing this on much smaller data sets. And the neural net is not a transformer, it's a multi layer Perceptron, but but it's the first time that a neural network has been applied in that setting. But even before neural networks, there were um language models except they were using um n gram models. So N gram models are just count based models. So um if you try to, if you start to take two words and predict a third one, you just count up how many times you've seen any uh two word combinations and what came next. what you predict that coming next is just what you've seen the most of in the training set. so uh language modeling has been around for a long time. Neural networks have done language modeling for a long time. So really what's new or interesting or exciting is just realizing that when you scale it up with a powerful enough neural net transformer, you have all these emergent properties where uh basically what happens is if you have a large enough data set of text. are in the task of predicting the next word. You are multitasking a huge amount of different kinds of problems. You are multitasking understanding of, you know, chemistry, physics, human nature, lots of things are sort of clustered in that objective. It's a very simple objective, but actually you have to understand a lot about the world to, to make that prediction."
}