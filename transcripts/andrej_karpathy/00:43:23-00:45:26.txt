{
  "Speaker": "Andrej Karpathy",
  "Start": "00:43:23",
  "End": "00:45:26",
  "Text": "? Think of it as a. So basically a transformer is a series of uh blocks, right? And these blocks have attention and a little multi layer perception. And so you you go off into a block and you come back to this residual pathway and then you go off and you come back and then you have a number of layers arranged sequentially. so the way to look at it I think is because of the residual pathway in the backward path, the gradients uh sort of flow allowing it uninterrupted because addition distributes the gradient equally to all of its branches. So the gradient from the supervision at the top just floats directly to the uh first layer and the all the residual connections are arranged so that in the beginning of during initialization, they contribute nothing to the residual pathway. Um what it kind of looks like is imagine the transformer is kind of like a Python function like a de and um you get to do various kinds of like lines of code. Um So you have 100 layers deep uh transformer typically they would be much shorter, say 20. So you have 20 lines of code, then you can do something in them. And so think of uh during the optimization basically what it looks like is first you optimize the first line of code and then the second line of code can kick in and the third line of code can kick in. And I kind of feel like because of the residual pathway and the dynamics of the optimization, you can sort of learn a very short algorithm that gets the approximate answer. But then the other layers can sort of kick in and start to create a contribution. And at the end of it, you're you're optimizing over an algorithm that is uh 20 lines of code these lines of code are very complex because it's an entire block of a transformer. You can do a lot in there. What's really interesting is that this transformer architecture actually has been a remarkably resilient. Basically, the transformer that came out in 2016 is the transformer you would use today, except you reshuffle some of the layer norms. Uh The layer normalization have been reshuffled to a pre norm um formulation. And so it's been remarkably stable, but there's a lot of bells and whistles that people have attached to it and try to improve it. do think that basically it's a, it's a big step in simultaneously optimizing for lots of properties of a desirable neural network architecture. And I think that people have been trying to change it, but it's proven remarkably resilient. Um But I do think that there should be even better architectures potentially, but"
}