{
  "Speaker": "Andrej Karpathy",
  "Start": "00:41:04",
  "End": "00:43:10",
  "Text": "want to have a general purpose computer that you can train on arbitrary problems uh like say the task of next word prediction or detecting if there's a cat in the image or something like that. you want to train this computer. So you want to set its its weights. And I think there's a number of design criteria that sort of overlap in the transformer simultaneously that made it very successful. And I think the authors were kind of um deliberately trying to uh make this really powerful architecture. And um so in a basically, it's very powerful in the forward pass because it's able to express um general uh computation as a sort of something that looks like message passing. You have nodes and they all store vectors. And uh these nodes get to basically look at each other and it's uh each other's vectors and they get to communicate and basically nodes get to broadcast, hey, I'm looking for things and then other nodes get to broadcast. Hey, these are the things I have, those are the keys and the values. So it's not just attention. Yeah, exactly. Transformer is much more than just the attention component. It's got many pieces architectural that went into it, the residual connection of the way it's arranged, there's a multi layer Perceptron and there, the way it's stacked and so on. Um But basically there's a message passing scheme where nodes get to look at each other, decide what's interesting and then update each other. uh so I think the um when you get to the details of it, I think it's a very expressive function. So it can express lots of different types of algorithms and forward pass. Not only that, but the way it's designed with the residual connections layer normalization, the soft matics attention and everything. It's also optimize. This is a really big deal because there's lots of computers that are powerful that you can't optimize um or they are not easy to optimize using the techniques that we have, which is back propagation ingredient and scent. These are first order methods, very simple optimizers really. And so um also needed to be optimized. Um And then lastly, you wanted to run efficiently in our hardware. Our hardware is a massive throughput machine like GP us. Uh They prefer lots of parallelism. you don't want to do lots of sequential operations, you want to do a lot of operations serially. And the transformer is designed with that in mind as well. And so it's designed for our hardware and is designed to both be very expressive in the forward pass but also very optimized in the backward pass."
}