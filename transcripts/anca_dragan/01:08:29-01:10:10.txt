{
  "Speaker": "Lex Fridman",
  "Start": "01:08:29",
  "End": "01:10:10",
  "Text": ", on that point, because I've gotten a huge amount of heat on this and I stand by it because I know the human factors community, well, the work here is really strong and there's many decades of work showing exactly what you're saying. Nevertheless, I've been continuously surprised that much of the predictions of that work has been wrong in what I've seen. So what we have to do, still agree with everything you said, but we have to be a little bit more minded. So I'll tell you, there's a few surprising things that like everything you said to the word is actually exactly correct. But it doesn't say what you didn't say is that these systems are, you said you can't assume a bunch of things, but we don't know if these systems are fundamentally unsafe, that's still unknown. There's a lot of interesting things like surprised by the fact, not the fact that what seems to be anecdotally well from large data collection that we've done. But also from just talking to a lot of people when in the supervisory role of semi-autonomous systems that are sufficiently dumb, at least, which is, that might be the key element is the systems have to be dumb. The people are actually more energized as observers. So they are actually better, better at observing the situation. So there might be cases in systems if you get the interaction right, where you as a supervisor will do a better job with the system together."
}