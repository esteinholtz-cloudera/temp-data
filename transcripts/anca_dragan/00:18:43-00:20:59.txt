{
  "Speaker": "Anca Dragan",
  "Start": "00:18:43",
  "End": "00:20:59",
  "Text": "the problem of take human behavior infer reward function from this to figure out what it is that that behavior is optimal with respect to. it's a great way to think about learning human preferences in the sense of, you know, you have a car and the person can drive it. And then you can say, well, OK, I can actually learn what the person is optimizing for. Um I can learn their driving style or you can have people demonstrate how they want the house clean. And then you can say OK, this is, this is I'm getting the tradeoffs that they're making, I'm getting the preferences that they want out of this. so we've been successful in robotics somewhat with this. And it's based on a very simple model of human behavior. It is remarkably simple, which is that human behavior is optimal respect to whatever it is that people want, right? So you make that assumption and now you can kind of inverse through that's why it's called inverse well, really optimal control but but also inverse reinforcement learning. this is based on utility maximization in economics. Whereas back in the forties, Von Neumann Morgenstein were like, OK, people are making choices by maximizing utility go. And then in the late fifties, had loose and shepherd come in and say people are a little bit noisy and approximate in that process. So they might choose something kind of stochastically with probability proportional to how much utility something has. So there's a bit of noise in there. This has translated into robotics and something that we call Boltzmann rationality. So it's a kind of an evolution of reinforcement learning that accounts for, for human noise. And we've had some success with that too for these tasks where it turns out people act noisily enough that you can't just do vanilla, the vanilla version, you can account for noise and still infer what what they seem to want on this then now we're hitting tasks where that's not enough. And"
}