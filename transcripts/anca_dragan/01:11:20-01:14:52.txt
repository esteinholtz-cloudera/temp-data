{
  "Speaker": "Anca Dragan",
  "Start": "01:11:20",
  "End": "01:14:52",
  "Text": "good question because the answer is we don't. this was um you know, I used to, I used to think about how, well, it's actually really hard to specify rewards for interaction because it's really supposed to be what the people want. And then, you know, we talked about how you have to customize what you want to do to the end user. But kind of realized that even if you take the interactive component away, still really hard to design reward functions. So what do I mean by that? I mean, if we assume this sort of a I paradigm in which there's an agent and his job is to optimize some objectives, some reward, utility loss, whatever cost. If you write it out, maybe it's a sad depending on the situation or whatever it is. Um If you write it out and then you deploy the agent, you'd want to make sure that whatever you specified incentivizes the behavior you want from the agent in any situation that the agent will be faced with. Right? So do motion planning on my robot arm. I specify some cost function like uh you know, this is how far away you should try to stay so much matter, stay away from people and this is how much it matters to be able to be efficient and blah, blah, blah, right? need to make sure that whatever I specify those constraints or trade offs or whatever they are when the robot goes and solves that problem in every new situation, that behavior is the behavior that I want to see. And what I've been finding is that we have no idea how to do that. That basically what I can do is I can sample, I can think of some situations that I think are representative of what the robot will face I can tune and add and tune some reward function until the optimal behavior is what I want on those situations. Which first of all is super frustrating because you know, through the miracle of A I, we, we don't have to specify rules for behavior anymore, right? You were saying before the robot comes up with the right thing to do, in this situation, it optimizes back in that situation. It optimizes, but you have to spend still a lot of time and actually defining what it is that, that criteria should be making sure you didn't forget about 50 bazillion things that are important and how they all should be combining together to tell the robot what's good and what's bad and how good and how bad. And so I think this is, this is a lesson that don't know, kind of, I guess I closed my eyes to it for a while because I've been tuning cost functions for 10 years now. Um But it, it really strikes me that, yeah, we've moved the tuning and the designing of features or whatever from the side into the reward side. And yes, I agree that there's way less of it, but it still seems really hard to anticipate any possible situation and make sure you specify a reward function that when optimized will work well in every possible situation."
}