{
  "Speaker": "Anca Dragan",
  "Start": "01:20:29",
  "End": "01:22:15",
  "Text": "true. Again, it comes back to, you can you structure a little bit your assumptions about how human behavior relates to what they want? Um And you know, you can, one thing that we've done is literally just treated this torque that they applied as you know, when you take that and you add it with what the torque the robot was already applying. That overall action is probably relatively optimal in respect to whatever it is that the person wants and then that gives you information about what it is that they want. So you can learn that people want you to stay further away from them. Now, you're right that there might be many things that explain just that one signal and that you might need much more data than that for, for, for the person to be able to shape your reward function over time. You can also do this info gathering stuff that we were talking about. Not that we've done that in that context just to clarify, but it's definitely something we thought about where you can have the robot start acting in a way if there are a bunch of different explanations, right? It moves in a way where it's easy if you correct it in some other way or not and then kind of actually plans its motion so that it can disambiguate and collect information about what you want. Anyway, so that's one way that's kind of sort of leaked information, maybe even more subtle, leaked information is if I just press the E stop, right? I just, I'm doing it out of panic because the robot is about to do something bad. again information there, right? Ok. The robot should definitely stop, but it should also figure out that whatever it was about to do was not good and in fact, it was so not good that stopping and remaining stopped for a while was better, a better trajectory for it than whatever it is that it was about to do. And that again is information about, what are my preferences? What do I want?"
}