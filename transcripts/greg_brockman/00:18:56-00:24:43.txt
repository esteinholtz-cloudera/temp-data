{
  "Speaker": "Greg Brockman",
  "Start": "00:18:56",
  "End": "00:24:43",
  "Text": "I think that in some ways it really boils down to taking a look at the landscape, right? So if you think about the history of A I that basically for the past 60 or 70 years, people have thought about this goal of what could happen if you could automate human intellectual labor. Right. you can build a computer system that could do that. What becomes possible. We have a lot of sci fi that tells stories of various dystopias and, you know, increasingly you have movies like her that tell you a little bit about maybe more of a little bit Utopic vision. Uh, you think about the impacts that we've seen from being able to, uh, have bicycles for our minds in computers. Uh, and that I think that the, the, the impact of, of computers and the internet has just far outstripped what anyone really could have predicted. so I think that it's very clear that if you can build an A G I, it will be the most transformative technology that humans will ever create. so what it boils down to then is a question of, well, is there a path, is there hope, is there a way to build such a system? And I think that for 60 or 70 years that people got excited and, uh, uh, that, you know, ended up not being able to deliver on the hopes that the, that people had pinned on them. I think that then, you know, that after, you know, 22 winters of A I development, uh, the people, uh, you know, I think kind of almost stopped daring to dream, right? That, that really talking about A G I or thinking about A G I became al almost this taboo in the community. I actually think that people took the wrong lesson from A I history. And if you look back starting in 1959 is when the Perceptron was released. And this is basically, you know, one of the earliest neural networks um it was released to what was perceived as this massive over hype. So in the New York Times, in 1959 you have this article uh saying that, you know, the, the Perceptron one day recognize people, call out their names, instantly translate speech between languages and people at the time, looked at this and said, this is your system can't do any of that and basically spent 10 years trying to discredit the whole Perceptron direction and, and succeeded and all the funding dried up and, you know, people kind of uh in other directions and you know, the eighties, there's a resurgence and I'd always heard that the resurgence in the eighties was due to the invention of back propagation and these these algorithms that got people excited. But actually, the causality was due to people building larger computers that you can find these, these articles from the eighties saying that the democratization of computing power meant that you could run these larger neural networks. And then people started to do all these amazing things. Back propagation algorithm was invented. And you know that the the neural nets people were running were these tiny little like 20 neuron neural nets, right? Like what are you supposed to learn with 20 neurons? And so of course, weren't able to get great results. And it really wasn't until 2012 that this approach, that's almost the most simple natural approach that people have come up with in the fifties, right? In some ways, even in the forties before there were computers with Pitts mccoll in there, neuron, this became the best way of solving problems, right? And I think there are three core properties that deep learning has that I think are very worth paying attention to. The first is generality, we have a very small number of deep learning tools. SGD deep neural net, maybe some, some you know RL it solves this huge variety of problems, speech recognition, machine translation game playing all of these problems, small set of tools. So there's the generality, there's a second piece which is the competence you wanna solve any of those problems throw up 40 years worth of normal computer vision research, replace it with a deep neural net, it's gonna work better. there's a third piece which is the scalability, right? That one thing that has been shown time and time again is that you if you have a larger neural network, throw more compute more data at it, it will work better. three properties together, feel like essential parts of building a general intelligence. Now, it doesn't just mean that if we scale up what we have that we will have an A G I right. There are clearly missing pieces, there are missing ideas, we need to have answers for reasoning. But I think that the core here is that for the first time, it feels that we have a paradigm that gives us hope that general intelligence can be achievable. so as soon as you believe that everything else becomes, comes into focus, right? If you imagine that you may be able to and you know, that the timeline, I think remains uncertain. Um that, but I think that that, you know, certainly within our lifetimes and possibly within a a much shorter period of time than, than people would expect if you can really build the most transformative technology that will ever exist, stop thinking about yourself so much, right? And you start thinking about just like, how do you have a world where this goes well? And that you need to think about the practicalities of how do you build an organization and get together a bunch of people and resources? Um and to make sure that people feel uh motivated and ready to, to do it. I think that then you start thinking about, well, what if we succeed? Um And how do we make sure that when we succeed, that the world is actually the place that, that, that we want ourselves to exist then and, you know, almost in the, the rosy and bell sense of the word. And so that's kind of the, the, the broader landscape and opening I was really formed in 2015 with that high level picture of A G I might be possible sooner than people think. And that uh we need to try to do our best to make sure it's going to go well. then we spent the next couple of years really trying to figure out what does that mean? How do we do it? Um And you know, I think that typically with a company, you start out very small. So you and a co-founder and you build a product, you get some users, you get product market fit, you know, then at some point you raise some money, you hire people, you scale and then, uh, you know, down the road, then the big companies realize you exist and try to kill you. Um And for open A I, it was basically everything in exactly the opposite order. Uh"
}