{
  "Speaker": "Greg Brockman",
  "Start": "00:45:45",
  "End": "00:50:36",
  "Text": ". So again, I think to zoom out like the way that we thought about GP T two is that with language modeling, we are clearly on a trajectory right now where we scale up our models and we get qualitatively better performance,. GP T two itself was actually just a scale up of a model that we've released in the previous June, right? And we just ran it at, you know, a much larger scale and we got these results where suddenly starting to write coherent pros uh which was not something we'd seen previously. what are we doing now? Well, we're gonna scale up GP D two by 10 X by 100 X by 1000 X and we don't know what we're gonna get. And so it's very clear that the model that we, that we released last June, you know, I think it's kind of like it's, it's, it's, it's a good academic toy. It's not something that we think is something that can really have negative applications or, you know, to the extent that it can, that the positive of people being able to play with it uh is, is, you know, far far outweighs the, the, the, the possible harms fast forward to not GP T two, but GP T 20. And you think about what that's gonna be like and I think that the capabilities are going to be substantive. And so there needs to be a point in between the two where you say this is something where we are drawing the line. Um and that we need to start thinking about the safety aspects. And I think for GP T two, we could have gone either way. And in fact, when we had conversations internally uh that we had a bunch of pros and cons um and it wasn't clear which one, which one outweighed the other. And I think that when we announced that, hey, we decide not to release this model. Um Then there was a bunch of conversation where various people said it's so obvious that you should have just released it. There are other people said it's so obvious you should not have released it. And I think that that almost definition means that holding it back was the correct decision, right? If it's con, if there's, if it's not obvious whether something is beneficial or not, you should probably default to caution. so I think that the, that the, that the overall landscape for how we think about it is that this decision could have gone either way. There's great arguments in both directions, but for future models down the road um, and possibly sooner than, than, than you'd expect because, you know, scaling these things up doesn't actually take that long. ones you're definitely not going to want to, to release into the wild. And so, um, I think that we, we almost view this as a test case and to see can we even design, you know, how, how do you have a society or how do you have a system that goes from having no concept of responsible disclosure where the mere idea of not releasing something for safety reasons is unfamiliar to a world where you say, OK, we have a powerful model. Let's at least think about it. Let's go through some process. you think about the security community, it took them a long time to design responsible disclosure, right? You know, you think about this question of, well, I have a security exploit. I send it to the company. The company is like tries to prosecute me or just sit, just ignores it. do I do? Right? And so, you know, the alternatives of, oh, I just, just always publish your exploits. That doesn't seem good either, right? And so it really took a long time and took this, this uh it was bigger than any individual, right? It's really about building a whole community that believe that OK, we'll have this process where you sent it to the company. You know, if they don't act at a certain time, then you can go public and you're not a bad person, you've done the right thing. Um And I think that in a I part of the, the response at GP D two just proves that we don't have any concept of this. So that's the high level picture. Um And so I think that I think this was, this was a really important move to make. Um And we could have maybe delayed it for GP T three, but I'm really glad we did it for GP T two. And so now you look at GP T two itself and you think about the substance of OK, what are potential negative applications? So you have this model that's been trained on the internet, which, you know, it's also going to be a bunch of very biased data, a bunch of, you know, very uh offensive content in there. Uh And uh you can ask it to generate for you on basically any topic, right? You just give it a prompt and it'll just start start writing and its content like you see on the internet, you know, even down to like saying advertisement in the middle of, of some of its generations. And uh you think about the possibilities for generating fake news or abusive content. you know, it's interesting seeing what people have done with, uh you know, we released a smaller version of GP T two and uh the people have done things like try to generate uh you know, to take my own Facebook message, history and generate more Facebook messages like me. Um and uh people generating fake politician uh uh content or uh you know, there, there's a bunch of, of things there where you at least have to think is this going to be good for the world? the flip side, which is, I think that there's a lot of awesome applications that we really want to see like creative uh applications in terms of if you have sci fi authors that can work with this tool and come with cool ideas. Like that seems that seems awesome if we can write better sci fi through the use of the, of these tools. And we've actually had a bunch of people write into us asking, hey, can we it for, you know, a variety of different creative applications? So"
}