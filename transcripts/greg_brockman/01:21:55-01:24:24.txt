{
  "Speaker": "Greg Brockman",
  "Start": "01:21:55",
  "End": "01:24:24",
  "Text": "? I'll stick to the, the kind of like the, the, the non grand answer first? Right? So the non grand answer is just to look at, you know, what are we already making work? You look at GP T two. A lot of people would have said that to even get these kinds of results. You need real world experience, you need a body, you need grounding. How are you supposed to reason about any of these things? How are you supposed to like, even kind of know about smoke and fire and those things if you've never experienced them? And GP D two shows that you can actually go way further than that kind of reasoning would predict. I think that the, the, in, in terms of, do we need consciousness, do we need a body? It seems the answer is probably not right, that we could probably just continue to push kind of the systems. We have, they already feel general. Um, they're not as competent or as general or able to learn as quickly as an A G I would. But, you know, they're at least like kind of proto A G I in some way and they don't need any of those things. Now, now let's move to the grand answer, which is, know, if are our neural nets nets conscious already, would we ever know? How can we tell? Right. And, you know, here, here's where the speculation starts to become, become, know, at least interesting or fun and maybe a little bit disturbing depending on, on where you take it. But it certainly seems that when we think about animals that there's some continuum of, of, of consciousness, you know, my cat I think is, uh is conscious in some way. Right. Uh you know, not as conscious as a human. And I mean, you could imagine that you could build a little conscious meter, right? You point at a cat, it gives you a little reading point at a human and it gives you much bigger reading. would happen if you pointed one of those at a Dota neural net. And if you're training in this massive simulation, do the neural nets feel pain? know, it becomes pretty hard to know that the answer is no. Um And it becomes pretty hard to, to really think about what that would mean if the answer were yes. it's very possible, you know, for example, you could imagine that maybe the reason that humans are have consciousness is because it's a, it's a convenient computational shortcut, right? If you think about it, if you have a being that wants to avoid pain, which seems pretty important to survive in this environment, um and wants to like, you know, eat food. Um then that maybe the best way of doing it is to have a being that's conscious, right? That, you know, in order to succeed in the environment, you need to have those properties and how are you supposed to implement them? And maybe this, this consciousness is way of doing that. that's true, then actually, maybe we should expect that really competent reinforcement learning agents will also have consciousness., you know, it's a big if and I think there are a lot of other arguments they can make in other directions. And I think"
}