{
  "Speaker": "Greg Brockman",
  "Start": "00:13:37",
  "End": "00:15:05",
  "Text": ", to answer the question, you can really look at how we structure open A I. So we really have three main arms. So we have capabilities which is actually doing the technical work and pushing forward what these systems can do. There's safety which is working on technical mechanisms to ensure that the systems we build are aligned with human values. And then there's policy which is making sure that we have governance mechanisms answering that question of well, whose values so I think that the technical safety one is the one that people kind of talk about the most, right? You talk about like think about, you know, all of the this topic A I movies, a lot of that is about not having good technical safety in place. Um, and what we've been finding is that, you know, I think that actually a lot of people look at the technical safety problem and think it's just intractable.. This question of what do humans want? How am I supposed to write that down? Can I even write down what I want? No way then they stop there. But the thing is we've already built systems that are able to learn things that humans can't specify, you know, even the rules for how to recognize if there's a cat or a dog in an image, turns out it's intractable to write that down and yet we're able to learn it that what we're seeing with systems, we build it open A I and they're still in early proof of concept stage is that you are able to learn human preferences, you're able to learn what humans want from data. Um And so that's kind of the core focus for our technical safety team. And I think that uh that they're actually we've had some pretty encouraging updates in terms of what we've been able to make work. So you,"
}