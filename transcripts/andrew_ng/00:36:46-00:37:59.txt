{
  "Speaker": "Andrew Ng",
  "Start": "00:36:46",
  "End": "00:37:59",
  "Text": "if you take the DL specialization, you learn the foundations of what is a neural network. How do you build up a neural network from a you know, single legacy unit to a stack of layers to um different activation functions? You learn how to train the neural networks. One thing I'm very proud of in that specialization is we go through a lot of practical know how of how to actually make these things work. So what are the differences between optimization algorithms? So what do you do if the algorithm over FTS or how do you tell the algorithm is overfitting? When do you collect more data? When should you not bother to collect more data? I find that um even today, unfortunately, there are, you know, engineers that will spend six months trying to pursue a political direction such as collect more data because we heard more data is valuable. But sometimes you could run some tests and could have figured out six months earlier that for this problem collecting more data isn't going to cut it. So just don't spend six months collecting more data, spend your time modifying the architecture or trying something else. So go through a lot of the practical know how so that when uh when, when, when someone, when you take the specialization, have those skills to be very efficient in how you build these networks to"
}