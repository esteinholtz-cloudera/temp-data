{
  "Speaker": "Andrew Ng",
  "Start": "00:46:28",
  "End": "00:48:51",
  "Text": "think that if my only job was being an academic researcher, have an unlimited budget and you know, didn't have to worry about short term impact and only focus on long term impact. I probably spend all my time doing research on Novis learning. I still think unsurprised learning is a beautiful idea um at both this past year and IC ML, I was attending workshops or listening to various talks about self supervised learning, which is one vertical segment, maybe a sort of unsured learning that I'm excited about. Uh maybe just to summarize the idea, I guess, you know, the idea of describe briefly. So here's an example of self supervised learning. say we grab a lot of unlabeled images off the internet. So with infinite amounts of this type of data, I'm going to take each image and rotate it by a random multiple of 90 degrees. And then I'm going to train a supervised neural network to predict what was the original orientation. So it has to be rotated 90 degrees, 180 degrees, 270 degrees or, or zero degrees. So you can generate an infinite amount of label data because you rotated the image. So you know what's the ground truth label. so uh various researchers have found that by taking on label data and making up label data sets and training a large neural network on these tasks, you can then take the hidden layer representation and transfer to a different task. Very powerfully um learning word embeddings where we take a sentence, delete the word predict the missing word, which is how we learn. You know, one of the ways we learn word embeddings another example. And I think um there's now this portfolio of techniques for generating these made up task. Um Another one called Jigsaw would be if you take an image, cut it up into a, you know, three by three grid. So like a 93 by three puzzle piece, jump out the nine pieces and have a neural network predict which of the nine factorial possible permutations it it came from. uh many groups including, you know, open A I Peter B has been doing some work on this to uh Facebook uh Google Brain. I think deep mind actually, Aaron Vander ad has great work on the C PC objective. So many teams are doing exciting work and I think this is a way generate infinite label data. Uh And, and I find this a very exciting piece of"
}