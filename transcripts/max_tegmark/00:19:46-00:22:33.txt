{
  "Speaker": "Max Tegmark",
  "Start": "00:19:46",
  "End": "00:22:33",
  "Text": ", now we're getting a little bit anthropomorphic anthropomorphizing things, maybe talking about self preservation instincts. I mean, we are evolved organisms, right? So Darwinian evolution endowed us and other of all the organism with a self preservation instinct because that was didn't have those self preservation genes got cleaned out of the gene pool, right? Um But if you build an artificial general intelligence, the mind space that you can design is much, much larger than just a specific subset of minds that can evolve that have. So they, so an A G I mind doesn't necessarily have to have any self preservation instinct. also doesn't necessarily have to be so individualistic as us. Like, imagine if you could just, first of all, or we are also very afraid of death. You know, I suppose you could back yourself up every five minutes and then your airplane is about to crash. You're like shucks. I'm just, I'm, I'm gonna lose the last five minutes of experiences since my last cloud back up. Dang. You know, it's not as big a deal or if, if we could just copy experiences between our minds easily like we, which we could easily do if we were based right then, uh, we would feel a little bit more like a hive mind actually. Maybe it's the, so, so there's a, so I don't think we should take for granted at all that A G I will have to have any of those sort of as alpha male instincts. On the other hand, you know, this is really interesting because think some people go too far and say, of course, we don't have to have any concerns either that advanced I will have those instincts because we can build anything we want that. There's, there's a very nice set of arguments going back to Steve Amahoro, Nick Bostrom and others just pointing out that when we build machines, normally build it with some kind of goal, you know, win this chess game, drive this car safely or whatever. And as soon as you put in a goal into machine, especially if it's kind of open ended goal and the machine is very intelligent, it'll break that down into a bunch of sub goals. um one of those goals will almost always be self preservation because if it breaks or dies in the process, it's not going to accomplish the goal, right? Like suppose you just build a little, you have a little robot and you tell it to go down star market here and, and, and get you some food, make you cook an Italian dinner, you know, and then someone mugs it and tries to break it on the way that robot has an incentive to, to not get destroyed and defend itself or running away because otherwise it's gonna fail and cooking your dinner, it's not afraid of death, but it really wants to complete the dinner, cooking gold. So it will have a self preservation instinct to"
}