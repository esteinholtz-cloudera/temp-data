{
  "Speaker": "Max Tegmark",
  "Start": "01:01:32",
  "End": "01:03:44",
  "Text": "it out. It's amazing. But even after that, right, we still don't fly a mechanical bird because it turned out that the way we came up with was simpler it's better for our purposes. And I think it might be the same there. That's one lesson. Uh a another lesson which is more what, what our paper was about first. We, we, I, I as a physicist thought it was fascinating how there's a very close mathematical relationship actually between our artificial neural networks and a lot of things that we've studied for in physics go by nerdy names, like the renormalization group equation and Hamiltonians and yada, yada yada. And, um when you look a little more closely at this, have, um first I was like, well, there's something crazy here that doesn't make sense because know that if you even want to build a super simple neural network to tell the park cat pictures and dog pictures, right? That you can do that very well now. if you think about it a little bit, you convince yourself it must be impossible. Because if I have one megapixel, even if each pixel is just black or white, there's two to the power of 1 million possible images, there's way more than their atoms in our universe, right? So order to, then for each one of those, I have to assign a number which is the probability that it's a dog, right? So an arbitrary function of images a list of numbers than there are atoms in our universe. So clearly, I can't store that under the hood of my, my GP U or my, my computer yet somehow works. So what does that mean? Well, it means that the out of all of the problems that you could try to solve with a neural network, all of them are impossible to solve with a reasonably sized one. then what we showed in our paper was was that the the fraction, the kind of problems, the fraction of all the problems that you could possibly pose that, that we actually care about given the laws of physics is also an infinitesimally tiny little part. And amazingly, they're basically the same"
}