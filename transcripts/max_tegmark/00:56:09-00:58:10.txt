{
  "Speaker": "Max Tegmark",
  "Start": "00:56:09",
  "End": "00:58:10",
  "Text": "check it out. Some of them are just mind blowing really beautiful. And, and you ask, how did it do that? got that talked to them as oa I know others from deep mind, they'll ultimately be able to give you is a big tables of numbers, matrices that define the neural network and you can stare at these tables numbers till your face turned blue. And it's, you're not gonna understand much about why it made that move. And uh even if you have a natural language processing, they can tell you in human language about 057 points 28 still not gonna really help. So I think, I think there's a whole spectrum of, of, of fun challenges there in and taking a computation that does intelligent things and transforming it into something, equally intelligent, it's more understandable. And I, I think that's really valuable because I think we put machines in charge of ever more infrastructure in our world, the power grid, the trading on the stock market, weapon systems and so on. It's crucial that we can trust these A is to do all we want and, and trust really comes from understanding, in a very fundamental way. And um that's why I'm, that's why I'm working on this because I think the more if we're gonna have some hope of of ensuring that machines have adopted our goals and that they're gonna retain them kind of trust, think needs to be based on things. You can actually understand preferably even make preferably to improve theorems on e even with a self driving car, right? someone just tells you it's been trained on tons of data and it never crashed. It's, it's less reassuring than if someone actually has a proof, maybe it's a computer verified proof, but still, it said that under no circumstances is this car just gonna swerve into oncoming traffic."
}