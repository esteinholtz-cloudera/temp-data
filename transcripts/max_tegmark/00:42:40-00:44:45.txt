{
  "Speaker": "Max Tegmark",
  "Start": "00:42:40",
  "End": "00:44:45",
  "Text": ", also, you know, another example which we can all relate to it of why it doesn't have to be a terrible thing to be in the presence of people are even smarter than us all around is when, when you and I were both two years old, I mean, our parents were much more intelligent than us, right? Worked out. Ok? Because their goals were aligned with our goals and that I think is really the number one key we have to solve if we value, align the value alignment problem. Exactly. Because people who see too many Hollywood movies with, with lousy science fiction, uh plot lines, they worry about the wrong thing, right? They worry about the machines only turning evil. not malice we, that is the concern, it's competence by definition, makes you, makes you very confident if you have a, a more intelligent go playing. Computer playing is the less intelligent one and, and when we define intelligence as the ability to accomplish, go winning. Right. It's gonna be the more intelligent one that wins. And if you have human and then you have an A G I that's more intelligent in all ways and they have different goals, guess who's going to get their way. Right. So, I was just reading about, I was just reading about this uh particular rhinoceros species that was driven extinct just a few years ago in a bummer. I was looking at this cute picture of a mommy rhinoceros with its its child. You know, and did we humans drive it to extinction? It wasn't because we were evil rhino haters as a whole. It was just because we, our goals weren't aligned with those of the rhinoceros and it didn't work out so well for the rhinoceros because we were more intelligent, right? So I think it's just so important that if we ever do build a G I we unleash anything, we have to make sure that it's, it learns to understand our goals that adopts our goals and it retains"
}